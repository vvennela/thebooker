{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d909bf91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguegamefinder, commonteamroster\n",
    "\n",
    "def determine_winning_team(row):\n",
    "    if row['PTS_HOME'] > row['PTS_AWAY']:\n",
    "        return row['HOME_TEAM']\n",
    "    elif row['PTS_HOME'] < row['PTS_AWAY']:\n",
    "        return row['AWAY_TEAM']\n",
    "\n",
    "def calculate_k_factor(winner_elo, loser_elo, margin_of_victory):\n",
    "    elo_difference = winner_elo - loser_elo\n",
    "    k_factor = 20 * ((margin_of_victory + 3) ** 0.08) / (7.5 + 0.006 * abs(elo_difference))\n",
    "    return k_factor\n",
    "\n",
    "def calculate_elo_rating(prev_elo, k_factor, result, opponent_elo, margin_of_victory):\n",
    "    expected_win_probability = 1 / (1 + 10 ** ((prev_elo - opponent_elo) / 400))\n",
    "    elo_change = k_factor * (result - expected_win_probability)\n",
    "    elo_change = elo_change.real\n",
    "    new_elo = round(float(prev_elo) + elo_change, 2)\n",
    "    return new_elo\n",
    "\n",
    "seasons = ['2014-15', '2015-16', '2016-17', \n",
    "           '2017-18', '2018-19', '2019-20', '2020-21', \n",
    "           '2021-22', '2022-23', '2023-24']\n",
    "\n",
    "combined_games_df = pd.DataFrame()\n",
    "\n",
    "team_names = ['Atlanta Hawks', 'Boston Celtics', 'Brooklyn Nets', 'Charlotte Hornets', 'Chicago Bulls', \n",
    "              'Cleveland Cavaliers', 'Dallas Mavericks', 'Denver Nuggets', 'Detroit Pistons', \n",
    "              'Golden State Warriors', 'Houston Rockets', 'Indiana Pacers', 'LA Clippers', 'Los Angeles Clippers',  \n",
    "              'Los Angeles Lakers', 'Memphis Grizzlies', 'Miami Heat', 'Milwaukee Bucks', \n",
    "              'Minnesota Timberwolves', 'New Orleans Pelicans', 'New York Knicks', \n",
    "              'Oklahoma City Thunder', 'Orlando Magic', 'Philadelphia 76ers', 'Phoenix Suns', \n",
    "              'Portland Trail Blazers', 'Sacramento Kings', 'San Antonio Spurs', 'Toronto Raptors', \n",
    "              'Utah Jazz', 'Washington Wizards']\n",
    "\n",
    "for season in seasons:\n",
    "    gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season, league_id_nullable='00', \n",
    "                                                   season_type_nullable='Regular Season', \n",
    "                                                   date_from_nullable='10/01/2014', date_to_nullable='04/30/2024')\n",
    "    games = gamefinder.get_data_frames()[0]\n",
    "    games = games.sort_values(by=['GAME_DATE'])\n",
    "\n",
    "    games['TEAM_NAME'] = games['TEAM_NAME'].replace({'Los Angeles Clippers': 'LA Clippers'})\n",
    "    games = games[games['TEAM_NAME'].str.lower().isin([name.lower() for name in team_names])]\n",
    "\n",
    "    home_team_data = games[games['MATCHUP'].str.contains('vs.')].copy()\n",
    "    away_team_data = games[games['MATCHUP'].str.contains('@')].copy()\n",
    "\n",
    "    aggregated_games = pd.merge(home_team_data, away_team_data, on=['GAME_ID'], suffixes=('_HOME', '_AWAY'))\n",
    "\n",
    "    home_cols = ['SEASON_ID_HOME', 'GAME_ID', 'GAME_DATE_HOME', 'TEAM_ABBREVIATION_HOME', 'PTS_HOME', 'MIN_HOME',\n",
    "                 'FGM_HOME', 'FGA_HOME', 'FG_PCT_HOME', 'FG3M_HOME', 'FG3A_HOME', 'FG3_PCT_HOME',\n",
    "                 'FTM_HOME', 'FTA_HOME', 'FT_PCT_HOME', 'OREB_HOME', 'DREB_HOME', 'REB_HOME',\n",
    "                 'AST_HOME', 'STL_HOME', 'BLK_HOME', 'TOV_HOME', 'PF_HOME', 'PLUS_MINUS_HOME', 'WL_HOME']\n",
    "\n",
    "    away_cols = ['SEASON_ID_AWAY', 'GAME_ID', 'GAME_DATE_AWAY', 'TEAM_ABBREVIATION_AWAY', 'PTS_AWAY', 'MIN_AWAY',\n",
    "                 'FGM_AWAY', 'FGA_AWAY', 'FG_PCT_AWAY', 'FG3M_AWAY', 'FG3A_AWAY', 'FG3_PCT_AWAY',\n",
    "                 'FTM_AWAY', 'FTA_AWAY', 'FT_PCT_AWAY', 'OREB_AWAY', 'DREB_AWAY', 'REB_AWAY',\n",
    "                 'AST_AWAY', 'STL_AWAY', 'BLK_AWAY', 'TOV_AWAY', 'PF_AWAY', 'PLUS_MINUS_AWAY', 'WL_AWAY']\n",
    "\n",
    "    games_df = pd.merge(aggregated_games[home_cols], aggregated_games[away_cols], on='GAME_ID', \n",
    "                        suffixes=('_HOME', '_AWAY'))\n",
    "\n",
    "    games_df['SEASON_ID'] = games_df['SEASON_ID_HOME'].combine_first(games_df['SEASON_ID_AWAY'])\n",
    "    games_df = games_df.drop(columns=['SEASON_ID_HOME', 'SEASON_ID_AWAY'])\n",
    "\n",
    "    games_df['HOME_TEAM'] = games_df['TEAM_ABBREVIATION_HOME']\n",
    "    games_df['AWAY_TEAM'] = games_df['TEAM_ABBREVIATION_AWAY']\n",
    "\n",
    "    games_df['WINNER'] = games_df.apply(determine_winning_team, axis=1)\n",
    "    column_names = [\n",
    "        'SEASON_ID', 'GAME_ID', 'GAME_DATE_HOME', 'HOME_TEAM', 'PTS_HOME', 'MIN_HOME',\n",
    "        'FGM_HOME', 'FGA_HOME', 'FG_PCT_HOME', 'FG3M_HOME', 'FG3A_HOME', 'FG3_PCT_HOME',\n",
    "        'FTM_HOME', 'FTA_HOME', 'FT_PCT_HOME', 'OREB_HOME', 'DREB_HOME', 'REB_HOME',\n",
    "        'AST_HOME', 'STL_HOME', 'BLK_HOME', 'TOV_HOME', 'PF_HOME', 'PLUS_MINUS_HOME', 'WL_HOME',\n",
    "        'GAME_DATE_AWAY', 'AWAY_TEAM', 'PTS_AWAY', 'MIN_AWAY', 'FGM_AWAY', 'FGA_AWAY',\n",
    "        'FG_PCT_AWAY', 'FG3M_AWAY', 'FG3A_AWAY', 'FG3_PCT_AWAY', 'FTM_AWAY', 'FTA_AWAY',\n",
    "        'FT_PCT_AWAY', 'OREB_AWAY', 'DREB_AWAY', 'REB_AWAY', 'AST_AWAY', 'STL_AWAY',\n",
    "        'BLK_AWAY', 'TOV_AWAY', 'PF_AWAY', 'PLUS_MINUS_AWAY', 'WL_AWAY'\n",
    "    ]\n",
    "\n",
    "    games_df = games_df[column_names]\n",
    "\n",
    "    games_df.rename(columns={'GAME_DATE_HOME': 'GAME_DATE'}, inplace=True)\n",
    "    games_df.drop(columns=['GAME_DATE_AWAY'], inplace=True)\n",
    "    games_df.drop(columns=['WL_HOME'], inplace=True)\n",
    "    games_df.drop(columns=['WL_AWAY'], inplace=True)\n",
    "    games_df['WINNING_TEAM'] = games_df.apply(determine_winning_team, axis=1)\n",
    "    games_df = games_df.sort_values(by=['GAME_DATE'])\n",
    "    combined_games_df = pd.concat([combined_games_df, games_df], ignore_index=True)\n",
    "\n",
    "data = pd.read_csv(\"games.csv\", index_col=0)\n",
    "initial_elo = 1500\n",
    "elo_ratings = {team: initial_elo for team in set(data['HOME_TEAM'])}\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    home_team = row['HOME_TEAM']\n",
    "    away_team = row['AWAY_TEAM']\n",
    "\n",
    "    home_team_elo = elo_ratings[home_team]\n",
    "    away_team_elo = elo_ratings[away_team]\n",
    "\n",
    "    if row['PTS_HOME'] > row['PTS_AWAY']:\n",
    "        winner_elo, loser_elo = home_team_elo, away_team_elo\n",
    "        result, margin_of_victory = 1, row['PTS_HOME'] - row['PTS_AWAY']\n",
    "    else:\n",
    "        winner_elo, loser_elo = away_team_elo, home_team_elo\n",
    "        result, margin_of_victory = 0, row['PTS_AWAY'] - row['PTS_HOME']\n",
    "\n",
    "    home_team_elo = calculate_elo_rating(home_team_elo, calculate_k_factor(winner_elo, loser_elo, margin_of_victory), result, loser_elo, margin_of_victory)\n",
    "    away_team_elo = calculate_elo_rating(away_team_elo, calculate_k_factor(winner_elo, loser_elo, -margin_of_victory), 1 - result, winner_elo, -margin_of_victory)\n",
    "    data.at[index, 'HOME_TEAM_ELO'] = home_team_elo\n",
    "    data.at[index, 'AWAY_TEAM_ELO'] = away_team_elo\n",
    "    elo_ratings[home_team] = home_team_elo\n",
    "    elo_ratings[away_team] = away_team_elo\n",
    "\n",
    "columns_to_keep = [\"GAME_ID\", \"HOME_TEAM_ELO\", \"AWAY_TEAM_ELO\"]\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "data['GAME_ID'] = data['GAME_ID'].astype(int)\n",
    "combined_games_df['GAME_ID'] = combined_games_df['GAME_ID'].astype(int)\n",
    "combined_games_df = pd.merge(combined_games_df, data, on=['GAME_ID'])\n",
    "\n",
    "combined_games_df.to_csv(\"games.csv\")\n",
    "\n",
    "\n",
    "df = combined_games_df\n",
    "home_data = df.groupby(['HOME_TEAM', 'SEASON_ID']).agg({\n",
    "    'PTS_HOME': 'mean',\n",
    "    'MIN_HOME': 'mean',\n",
    "    'FGM_HOME': 'mean',\n",
    "    'FGA_HOME': 'mean',\n",
    "    'FG_PCT_HOME': 'mean',\n",
    "    'FG3M_HOME': 'mean',\n",
    "    'FG3A_HOME': 'mean',\n",
    "    'FG3_PCT_HOME': 'mean',\n",
    "    'FTM_HOME': 'mean',\n",
    "    'FTA_HOME': 'mean',\n",
    "    'FT_PCT_HOME': 'mean',\n",
    "    'OREB_HOME': 'mean',\n",
    "    'DREB_HOME': 'mean',\n",
    "    'REB_HOME': 'mean',\n",
    "    'AST_HOME': 'mean',\n",
    "    'STL_HOME': 'mean',\n",
    "    'BLK_HOME': 'mean',\n",
    "    'TOV_HOME': 'mean',\n",
    "    'PF_HOME': 'mean',\n",
    "    'PLUS_MINUS_HOME': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "home_data.rename(columns={\n",
    "    'PTS_HOME': 'Average_PTS_Home',\n",
    "    'MIN_HOME': 'Average_MIN_Home',\n",
    "    'FGM_HOME': 'Average_FGM_Home',\n",
    "    'FGA_HOME': 'Average_FGA_Home',\n",
    "    'FG_PCT_HOME': 'Average_FG_PCT_Home',\n",
    "    'FG3M_HOME': 'Average_FG3M_Home',\n",
    "    'FG3A_HOME': 'Average_FG3A_Home',\n",
    "    'FG3_PCT_HOME': 'Average_FG3_PCT_Home',\n",
    "    'FTM_HOME': 'Average_FTM_Home',\n",
    "    'FTA_HOME': 'Average_FTA_Home',\n",
    "    'FT_PCT_HOME': 'Average_FT_PCT_Home',\n",
    "    'OREB_HOME': 'Average_OREB_Home',\n",
    "    'DREB_HOME': 'Average_DREB_Home',\n",
    "    'REB_HOME': 'Average_REB_Home',\n",
    "    'AST_HOME': 'Average_AST_Home',\n",
    "    'STL_HOME': 'Average_STL_Home',\n",
    "    'BLK_HOME': 'Average_BLK_Home',\n",
    "    'TOV_HOME': 'Average_TOV_Home',\n",
    "    'PF_HOME': 'Average_PF_Home',\n",
    "    'PLUS_MINUS_HOME': 'Average_PLUS_MINUS_Home',  \n",
    "}, inplace=True)\n",
    "\n",
    "away_data = df.groupby(['AWAY_TEAM', 'SEASON_ID']).agg({\n",
    "    'PTS_AWAY': 'mean',\n",
    "    'MIN_AWAY': 'mean',\n",
    "    'FGM_AWAY': 'mean',\n",
    "    'FGA_AWAY': 'mean',\n",
    "    'FG_PCT_AWAY': 'mean',\n",
    "    'FG3M_AWAY': 'mean',\n",
    "    'FG3A_AWAY': 'mean',\n",
    "    'FG3_PCT_AWAY': 'mean',\n",
    "    'FTM_AWAY': 'mean',\n",
    "    'FTA_AWAY': 'mean',\n",
    "    'FT_PCT_AWAY': 'mean',\n",
    "    'OREB_AWAY': 'mean',\n",
    "    'DREB_AWAY': 'mean',\n",
    "    'REB_AWAY': 'mean',\n",
    "    'AST_AWAY': 'mean',\n",
    "    'STL_AWAY': 'mean',\n",
    "    'BLK_AWAY': 'mean',\n",
    "    'TOV_AWAY': 'mean',\n",
    "    'PF_AWAY': 'mean',\n",
    "    'PLUS_MINUS_AWAY': 'mean', \n",
    "}).reset_index()\n",
    "\n",
    "away_data.rename(columns={\n",
    "    'PTS_AWAY': 'Average_PTS_Away',\n",
    "    'MIN_AWAY': 'Average_MIN_Away',\n",
    "    'FGM_AWAY': 'Average_FGM_Away',\n",
    "    'FGA_AWAY': 'Average_FGA_Away',\n",
    "    'FG_PCT_AWAY': 'Average_FG_PCT_Away',\n",
    "    'FG3M_AWAY': 'Average_FG3M_Away',\n",
    "    'FG3A_AWAY': 'Average_FG3A_Away',\n",
    "    'FG3_PCT_AWAY': 'Average_FG3_PCT_Away',\n",
    "    'FTM_AWAY': 'Average_FTM_Away',\n",
    "    'FTA_AWAY': 'Average_FTA_Away',\n",
    "    'FT_PCT_AWAY': 'Average_FT_PCT_Away',\n",
    "    'OREB_AWAY': 'Average_OREB_Away',\n",
    "    'DREB_AWAY': 'Average_DREB_Away',\n",
    "    'REB_AWAY': 'Average_REB_Away',\n",
    "    'AST_AWAY': 'Average_AST_Away',\n",
    "    'STL_AWAY': 'Average_STL_Away',\n",
    "    'BLK_AWAY': 'Average_BLK_Away',\n",
    "    'TOV_AWAY': 'Average_TOV_Away',\n",
    "    'PF_AWAY': 'Average_PF_Away',\n",
    "    'PLUS_MINUS_AWAY': 'Average_PLUS_MINUS_Away', \n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "average = pd.merge(home_data, away_data, left_on=['HOME_TEAM', 'SEASON_ID'], \n",
    "                   right_on=['AWAY_TEAM', 'SEASON_ID'], suffixes=('_HOME', '_AWAY'))\n",
    "\n",
    "cols_to_avg = ['PTS', 'MIN', 'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', \n",
    "               'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', \n",
    "               'AST', 'STL', 'BLK', 'TOV', 'PF', 'PLUS_MINUS']\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    average[f'Average_{col}'] = (average[f'Average_{col}_Home'] + average[f'Average_{col}_Away']) / 2\n",
    "    \n",
    "average = average[['HOME_TEAM', 'SEASON_ID'] + [f'Average_{col}' for col in cols_to_avg]]\n",
    "\n",
    "average.rename(columns={\n",
    "    'HOME_TEAM' : 'TEAM' \n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "average.to_csv(\"game_averages.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a8d819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# performance for each player's performance this season + moving average\n",
    "\n",
    "from nba_api.stats.endpoints import leaguegamefinder, commonteamroster, PlayerGameLog\n",
    "import pandas as pd\n",
    "from requests.exceptions import ReadTimeout\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def retry_request(request_func, max_retries=3, timeout=60, delay=0):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return request_func()\n",
    "        except ReadTimeout:\n",
    "            print(f\"delay\")\n",
    "            time.sleep(delay)\n",
    "            retries += 1\n",
    "    raise Exception(\"can't retry\")\n",
    "\n",
    "\n",
    "season = \"2023-24\"\n",
    "game_finder = leaguegamefinder.LeagueGameFinder(season_nullable=\"2023-24\")\n",
    "games_data = retry_request(game_finder.get_data_frames)[0]\n",
    "\n",
    "all_team_ids = list(set(games_data['TEAM_ID'].unique()))\n",
    "\n",
    "all_rosters = pd.DataFrame()\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "for i in range(0, len(all_team_ids), batch_size):\n",
    "    batch_team_ids = all_team_ids[i:i+batch_size]\n",
    "    for team_id in batch_team_ids:\n",
    "        roster_data = retry_request(lambda: commonteamroster.CommonTeamRoster(team_id=team_id, season=season).get_data_frames()[0])\n",
    "        all_rosters = all_rosters.append(roster_data, ignore_index=True)\n",
    "        time.sleep(.125)\n",
    "\n",
    "unique_players = all_rosters['PLAYER_ID'].unique()\n",
    "\n",
    "all_player_logs = pd.DataFrame()\n",
    "last_10_player_logs = pd.DataFrame()\n",
    "\n",
    "for player_id in unique_players:\n",
    "    try:\n",
    "        player_game_log = PlayerGameLog(player_id=player_id, season=season, season_type_all_star='Regular Season')\n",
    "        player_game_log_data = player_game_log.get_data_frames()[0]\n",
    "        player_game_log_data = player_game_log_data.sort_values(by='GAME_DATE', ascending=True)\n",
    "        all_player_logs = all_player_logs.append(player_game_log_data, ignore_index=True)\n",
    "        last_10_games_data = player_game_log_data.head(10)\n",
    "        last_10_player_logs = last_10_player_logs.append(last_10_games_data, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for player_id {player_id}: {str(e)}\")\n",
    "    time.sleep(.125)\n",
    "\n",
    "all_player_logs.to_csv(\"all_games.csv\", index=False)\n",
    "last_10_player_logs.to_csv(\"last10games.csv\", index=False)\n",
    "\n",
    "# Calculate averages for last 10 games\n",
    "moving_average = pd.read_csv(\"last10games.csv\", index_col=[0])\n",
    "columns_to_drop = [\"Game_ID\", \"GAME_DATE\", \"MATCHUP\", \"WL\", \"VIDEO_AVAILABLE\"]\n",
    "moving_average = moving_average.drop(columns=columns_to_drop)\n",
    "moving_average.to_csv(\"player_avg_last10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17e7390c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5682\n"
     ]
    }
   ],
   "source": [
    "## highest_accuracy model yet\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "ml_data = pd.read_csv(\"games.csv\")\n",
    "ml_data = ml_data.drop(['GAME_DATE'], axis=1)\n",
    "target = ml_data['WINNING_TEAM']\n",
    "\n",
    "features = ml_data.copy()\n",
    "home_cols = ['HOME_TEAM_ELO', 'PTS_HOME', 'MIN_HOME', 'FGM_HOME', 'FGA_HOME', 'FG_PCT_HOME', 'FG3M_HOME', 'FG3A_HOME', 'FG3_PCT_HOME',\n",
    "             'FTM_HOME', 'FTA_HOME', 'FT_PCT_HOME', 'OREB_HOME', 'DREB_HOME', 'REB_HOME', 'AST_HOME', 'STL_HOME', 'BLK_HOME', 'TOV_HOME', 'PF_HOME']\n",
    "\n",
    "away_cols = ['AWAY_TEAM_ELO', 'PTS_AWAY', 'MIN_AWAY', 'FGM_AWAY', 'FGA_AWAY', 'FG_PCT_AWAY', 'FG3M_AWAY', 'FG3A_AWAY', 'FG3_PCT_AWAY',\n",
    "             'FTM_AWAY', 'FTA_AWAY', 'FT_PCT_AWAY', 'OREB_AWAY', 'DREB_AWAY', 'REB_AWAY', 'AST_AWAY', 'STL_AWAY', 'BLK_AWAY', 'TOV_AWAY', 'PF_AWAY']\n",
    "\n",
    "for col in home_cols:\n",
    "    features[col + '_DIFF'] = features[col] - features[col.replace('HOME', 'AWAY')]\n",
    "\n",
    "features = features.drop(home_cols + away_cols, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "categorical_cols = ['HOME_TEAM', 'AWAY_TEAM']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_cols),\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter = 100000))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "# print(f'Classification Report:\\n{classification_rep}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915dacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS\n",
    "\n",
    "def calculate_elo(df):\n",
    "    before_elo_ratings = {}  \n",
    "    after_elo_ratings = {}   \n",
    "    before_elo_list = []\n",
    "    after_elo_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        team = row['TEAM']\n",
    "        opponent_team = row['OPP']\n",
    "        before_elo_team = before_elo_ratings.get(team, 1500)\n",
    "        before_elo_opponent = before_elo_ratings.get(opponent_team, 1500)\n",
    "\n",
    "    # Check if it's the first game in the dataset\n",
    "        if row.name == 0:\n",
    "        # Calculate ELO using the specified equation for the first game\n",
    "            k_factor = 20 * ((abs(row['PLUS_MINUS']) + 3) ** 0.08) / (7.5 + 0.006 * abs(before_elo_team - before_elo_opponent))\n",
    "            expected_win = 1 / (1 + 10**((before_elo_opponent - before_elo_team) / 400))\n",
    "            after_elo_game = before_elo_team + k_factor * (int(row['WL'] == 'W') - expected_win)\n",
    "        else:\n",
    "            k_factor = 20 * ((abs(row['PLUS_MINUS']) + 3) ** 0.08) / (7.5 + 0.006 * abs(before_elo_team - before_elo_opponent))\n",
    "            expected_win = 1 / (1 + 10**((before_elo_opponent - before_elo_team) / 400))\n",
    "            after_elo_game = before_elo_team + k_factor * (int(row['WL'] == 'W') - expected_win)\n",
    "\n",
    "        # Update before_elo_ratings with after_elo_game\n",
    "        before_elo_ratings[team] = after_elo_game\n",
    "        after_elo_ratings[team] = after_elo_game\n",
    "        after_elo_list.append(after_elo_game)\n",
    "        before_elo_list.append(before_elo_team)\n",
    "\n",
    "    df['BEFORE_ELO'] = before_elo_list\n",
    "    df['AFTER_ELO'] = after_elo_list\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## def calculate_moving_average(df, team_name, date_of_game, stats, max_window_size=10):\n",
    "#    team_df = df[df['TEAM'] == team_name].sort_values(by='GAME_DATE')\n",
    "#    team_df = team_df[team_df['GAME_DATE'] < date_of_game]\n",
    "#    window_size = min(len(team_df), max_window_size)\n",
    "#    if window_size <= 0:\n",
    "#        return df[(df['TEAM'] == team_name) & (df['GAME_DATE'] == date_of_game)][stats].iloc[0]\n",
    "#    moving_averages = team_df[stats].rolling(window=window_size, min_periods=1).mean().shift(1).iloc[-1]\n",
    "#    return moving_averages\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguegamefinder, commonteamroster\n",
    "\n",
    "seasons = ['2014-15', '2015-16', '2016-17', \n",
    "           '2017-18', '2018-19', '2019-20', '2020-21', \n",
    "           '2021-22', '2022-23', '2023-24']\n",
    "\n",
    "games_df = pd.DataFrame()\n",
    "\n",
    "selected_stats = ['PTS', 'FGM', 'FGA', 'FG_PCT',\n",
    "                  'FG3M', 'FG3A', 'FG3_PCT', 'FTM',\n",
    "                  'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB',\n",
    "                  'AST', 'STL', 'BLK', 'TOV', 'PF', 'PLUS_MINUS']\n",
    "\n",
    "\n",
    "\n",
    "team_names = ['Atlanta Hawks', 'Boston Celtics', 'Brooklyn Nets', 'Charlotte Hornets', 'Chicago Bulls', \n",
    "              'Cleveland Cavaliers', 'Dallas Mavericks', 'Denver Nuggets', 'Detroit Pistons', \n",
    "              'Golden State Warriors', 'Houston Rockets', 'Indiana Pacers', 'LA Clippers', 'Los Angeles Clippers',  \n",
    "              'Los Angeles Lakers', 'Memphis Grizzlies', 'Miami Heat', 'Milwaukee Bucks', \n",
    "              'Minnesota Timberwolves', 'New Orleans Pelicans', 'New York Knicks', \n",
    "              'Oklahoma City Thunder', 'Orlando Magic', 'Philadelphia 76ers', 'Phoenix Suns', \n",
    "              'Portland Trail Blazers', 'Sacramento Kings', 'San Antonio Spurs', 'Toronto Raptors', \n",
    "              'Utah Jazz', 'Washington Wizards']\n",
    "\n",
    "for season in seasons:\n",
    "    gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season, league_id_nullable='00', \n",
    "                                                   season_type_nullable='Regular Season', \n",
    "                                                   date_from_nullable='10/01/2014', date_to_nullable='04/30/2024')\n",
    "    games = gamefinder.get_data_frames()[0]\n",
    "    games = games.sort_values(by=['GAME_DATE'])\n",
    "\n",
    "    games['TEAM_NAME'] = games['TEAM_NAME'].replace({'Los Angeles Clippers': 'LA Clippers'})\n",
    "    games = games[games['TEAM_NAME'].str.lower().isin([name.lower() for name in team_names])]\n",
    "    \n",
    "    games_df = pd.concat([games_df, games], ignore_index=True)\n",
    "\n",
    "games_df['MATCHUP'] = games_df['MATCHUP'].str[-3:]\n",
    "games_df = games_df.rename(columns={'MATCHUP': 'OPP', \"TEAM_ABBREVIATION\": \"TEAM\"})\n",
    "games_df = games_df.drop(columns=['TEAM_NAME', 'MIN'])\n",
    "games_df = round(calculate_elo(games_df),2)\n",
    "\n",
    "games_df['GAME_DATE'] = pd.to_datetime(games_df['GAME_DATE'])\n",
    "\n",
    "for stat in selected_stats:\n",
    "    games_df[f\"SLIDING_{stat}\"] = games_df.groupby('TEAM')[stat].transform(lambda x: x.rolling(window=10,                                                                       min_periods=1).mean().shift())\n",
    "for stat in selected_stats:\n",
    "    games_df.loc[games_df.groupby('TEAM').cumcount() <= 1, f\"SLIDING_{stat}\"] = games_df[stat]\n",
    "games_df = games_df.round({'SLIDING_' + stat: 2 for stat in selected_stats})\n",
    "\n",
    "games_df.to_csv(\"newGames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65145663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with Best Hyperparameters: 0.5838277301691935\n",
      "Best Hyperparameters: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"newGames.csv\")\n",
    "\n",
    "df_filtered = df[df['WL'] != 2]\n",
    "\n",
    "# Define features and label\n",
    "features = ['BEFORE_ELO', 'SLIDING_PTS', 'SLIDING_FGM', 'SLIDING_FGA', 'SLIDING_FG_PCT',\n",
    "            'SLIDING_FG3M', 'SLIDING_FG3A', 'SLIDING_FG3_PCT', 'SLIDING_FTM', 'SLIDING_FTA',\n",
    "            'SLIDING_FT_PCT', 'SLIDING_OREB', 'SLIDING_DREB', 'SLIDING_REB', 'SLIDING_AST',\n",
    "            'SLIDING_STL', 'SLIDING_BLK', 'SLIDING_TOV', 'SLIDING_PF', 'SLIDING_PLUS_MINUS']\n",
    "\n",
    "label = 'WL'  # Assuming 'WL' is your target label\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_data, test_data = df[:train_size], df[train_size:]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[label].apply(lambda x: 1 if x == 'W' else 0)\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[label].apply(lambda x: 1 if x == 'W' else 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_logreg = LogisticRegression(**best_params, random_state=42)\n",
    "best_logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions = best_logreg.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy with Best Hyperparameters: {accuracy}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee25b412",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4f/r4sxs9j97fdf1c4x4r1wxlt40000gn/T/ipykernel_13421/4026313906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mbest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    843\u001b[0m                     )\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    846\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \"\"\"\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    441\u001b[0m             )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"games.csv\")\n",
    "df_filtered = df[df['WL'] != 2]\n",
    "\n",
    "features = ['BEFORE_ELO', 'SLIDING_PTS', 'SLIDING_FGM', 'SLIDING_FGA', 'SLIDING_FG_PCT',\n",
    "            'SLIDING_FG3M', 'SLIDING_FG3A', 'SLIDING_FG3_PCT', 'SLIDING_FTM', 'SLIDING_FTA',\n",
    "            'SLIDING_FT_PCT', 'SLIDING_OREB', 'SLIDING_DREB', 'SLIDING_REB', 'SLIDING_AST',\n",
    "            'SLIDING_STL', 'SLIDING_BLK', 'SLIDING_TOV', 'SLIDING_PF', 'SLIDING_PLUS_MINUS']\n",
    "\n",
    "label = 'WL'  \n",
    "le = LabelEncoder()\n",
    "df[label] = le.fit_transform(df[label])\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[label]\n",
    "\n",
    "X_validation = validation_data[features]\n",
    "y_validation = validation_data[label]\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[label]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'max_depth': [8, 11, 14],\n",
    "    'min_samples_split': [8, 11, 14],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=1))\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dfe7544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5884\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.54      0.57      2293\n",
      "           1       0.58      0.64      0.61      2260\n",
      "\n",
      "    accuracy                           0.59      4553\n",
      "   macro avg       0.59      0.59      0.59      4553\n",
      "weighted avg       0.59      0.59      0.59      4553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## USE THIS \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"newGames.csv\")\n",
    "df = df[df['WL'].isin(['W', 'L'])]\n",
    "\n",
    "features = ['BEFORE_ELO', 'SLIDING_PTS', 'SLIDING_FGM', 'SLIDING_FGA', 'SLIDING_FG_PCT',\n",
    "            'SLIDING_FG3M', 'SLIDING_FG3A', 'SLIDING_FG3_PCT', 'SLIDING_FTM', 'SLIDING_FTA',\n",
    "            'SLIDING_FT_PCT', 'SLIDING_OREB', 'SLIDING_DREB', 'SLIDING_REB', 'SLIDING_AST',\n",
    "            'SLIDING_STL', 'SLIDING_BLK', 'SLIDING_TOV', 'SLIDING_PF', 'SLIDING_PLUS_MINUS']\n",
    "\n",
    "label = 'WL'  \n",
    "le = LabelEncoder()\n",
    "df[label] = le.fit_transform(df[label])\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[label]\n",
    "\n",
    "X_validation = validation_data[features]\n",
    "y_validation = validation_data[label]\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[label]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9018e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities for Team 1: [0.30763766 0.69236234]\n",
      "Predicted Class for Team 1: W\n",
      "Predicted Probabilities for Team 2: [0.40238306 0.59761694]\n",
      "Predicted Class for Team 2: W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25066fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50447191 0.49552809]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bea0241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc55fa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner is: GSW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "team_name = 'GSW'  # Replace 'YourTeam' with the actual team name\n",
    "opp_name = 'NOP'\n",
    "# Filter the dataset for the specified team\n",
    "team_data1 = df[df['TEAM'] == team_name]\n",
    "team_data2 = df[df['TEAM'] == opp_name]\n",
    "team_data1 = team_data1[features]\n",
    "team_data2 = team_data2[features]\n",
    "\n",
    "team_data1_array = team_data1[features].values[-1].reshape(1, -1)\n",
    "team_data2_array = team_data2[features].values[-1].reshape(1, -1)\n",
    "\n",
    "# Set feature names for RandomForestClassifier\n",
    "best_rf.feature_names_in_ = features\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities_team1 = best_rf.predict_proba(team_data1_array)[0][1]  # Probability for class 1 (win)\n",
    "probabilities_team2 = best_rf.predict_proba(team_data2_array)[0][1]  # Probability for class 1 (win)\n",
    "\n",
    "# Determine the winner based on the higher probability\n",
    "winner = team_name if probabilities_team1 > probabilities_team2 else opp_name\n",
    "\n",
    "# Display the results\n",
    "#print(f\"Predicted Probability for {team_name}: {probabilities_team1:.4f}\")\n",
    "#print(f\"Predicted Probability for {opp_name}: {probabilities_team2:.4f}\")\n",
    "print(f\"The winner is: {winner}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945adbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24702a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c50940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
